{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "asl_cnn",
      "provenance": [],
      "authorship_tag": "ABX9TyPr8tmQf5DysRfnkjo8gfcH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rmbayer/ASL_NN/blob/master/asl_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6uupcCJN5RJ",
        "colab_type": "text"
      },
      "source": [
        "#CNN TensorFlow Framework for American Sign Language Detection\n",
        "### **Bob Bayer and Jonathan Rice 2020**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "901jIDuWOP2K",
        "colab_type": "text"
      },
      "source": [
        "**Retrieve Datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBgVXfJmN4Mq",
        "colab_type": "code",
        "outputId": "8e7d6ef6-3474-4374-d913-29b57385a223",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# Import and mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pzvvfm25UT84",
        "colab_type": "code",
        "outputId": "a294fa4c-2cf1-4896-85b9-61516a531b26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Check drive contents\n",
        "!ls \"/content/drive/My Drive/Sign Language\""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " asl-alphabet.zip\t\t      'Sign Language Project Research.gdoc'\n",
            "'ASL Recognition with Deep Learning'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAwCZynId9_z",
        "colab_type": "text"
      },
      "source": [
        "**The following python code will use the OS library to use Operating System libraries, giving you access to the file system, and the zipfile library allowing you to unzip the data.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4vpEj2QQ--B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/content/drive/My Drive/Sign Language/asl-alphabet.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "# Extract to a local file and close the connection to the drive file\n",
        "zip_ref.extractall('/tmp/') \n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9ZihcBRd4yf",
        "colab_type": "text"
      },
      "source": [
        "**Set the directories for the training and test datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxIs9CHgeNcw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dir = os.path.join('/tmp/asl_alphabet_train/asl_alphabet_train')\n",
        "test_dir= os.path.join('/tmp/asl_alphabet_test/asl_alphabet_test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHU9EFzmogF-",
        "colab_type": "text"
      },
      "source": [
        "**Veryify the subfolders of the directory**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1Vsb51mfRb9",
        "colab_type": "code",
        "outputId": "e558ddee-946c-4179-907d-f57bead6623f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "train_names = os.listdir(train_dir)\n",
        "print(train_names)\n",
        "test_names = os.listdir(test_dir)\n",
        "print(test_names)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['A', 'G', 'S', 'W', 'N', 'L', 'H', 'X', 'K', 'V', 'P', 'I', 'T', 'B', 'C', 'R', 'del', 'J', 'F', 'D', 'Z', 'space', 'E', 'Q', 'M', 'U', 'Y', 'nothing', 'O']\n",
            "['J_test.jpg', 'N_test.jpg', 'P_test.jpg', 'X_test.jpg', 'A_test.jpg', 'T_test.jpg', 'Z_test.jpg', 'G_test.jpg', 'H_test.jpg', 'C_test.jpg', 'Y_test.jpg', 'W_test.jpg', 'E_test.jpg', 'space_test.jpg', 'U_test.jpg', 'K_test.jpg', 'D_test.jpg', 'M_test.jpg', 'S_test.jpg', 'B_test.jpg', 'R_test.jpg', 'L_test.jpg', 'V_test.jpg', 'I_test.jpg', 'nothing_test.jpg', 'F_test.jpg', 'O_test.jpg', 'Q_test.jpg']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZnqjBM3rTN5",
        "colab_type": "text"
      },
      "source": [
        "**Build the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIpch_TerZq4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1WW83M3oROD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 300x300 with 3 bytes color\n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(200, 200, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    #The fourth convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    #The fifth convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    # 128 neuron hidden layer\n",
        "    tf.keras.layers.Dense(128, activation = tf.nn.relu),\n",
        "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')\n",
        "    tf.keras.layers.Dense(29, activation = tf.nn.softmax)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZS0BKMRPtbAI",
        "colab_type": "text"
      },
      "source": [
        "**Inspect the Layers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r12aDWCfrfFd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "a5121d8b-b3fe-47b7-91af-5559c42a8d69"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 198, 198, 16)      448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 99, 99, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 97, 97, 32)        4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 48, 48, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 46, 46, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 23, 23, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 21, 21, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 10, 10, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 8, 8, 64)          36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               131200    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 29)                3741      \n",
            "=================================================================\n",
            "Total params: 232,381\n",
            "Trainable params: 232,381\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d-RqS69teGY",
        "colab_type": "text"
      },
      "source": [
        "**Set the optimizer** <br>\n",
        "Use adam for it's adaptive learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bQcbgK6rfIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5upIJU0o8I7",
        "colab_type": "text"
      },
      "source": [
        "**Define the ImageGenerator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cslF61L7kSiE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9e4c78a5-765b-41b7-99b1-7b538d784a31"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,  # This is the source directory for training images\n",
        "        target_size=(200, 200),\n",
        "        batch_size=100,\n",
        "        # Since we use categorical_crossentropy loss, we need sparse labels\n",
        "        class_mode='sparse')\n",
        "\n",
        "# May not need this\n",
        "label_map = (train_generator.class_indices)\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 87000 images belonging to 29 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZWd72e7nITM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0a48d86b-65f2-4f8b-a58b-af4713b417f8"
      },
      "source": [
        "print(label_map)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'J': 9, 'K': 10, 'L': 11, 'M': 12, 'N': 13, 'O': 14, 'P': 15, 'Q': 16, 'R': 17, 'S': 18, 'T': 19, 'U': 20, 'V': 21, 'W': 22, 'X': 23, 'Y': 24, 'Z': 25, 'del': 26, 'nothing': 27, 'space': 28}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QodatZkxtWPl",
        "colab_type": "text"
      },
      "source": [
        "**Train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b82HI0xStU-b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "053be925-4db9-4704-9b2f-c440df00f638"
      },
      "source": [
        "model.fit(train_generator, \n",
        "          steps_per_epoch = 870, #we have 87,000 images, batch size of 100\n",
        "          epochs=10)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "870/870 [==============================] - 142s 163ms/step - loss: 0.9226 - accuracy: 0.7188\n",
            "Epoch 2/10\n",
            "870/870 [==============================] - 141s 163ms/step - loss: 0.1094 - accuracy: 0.9636\n",
            "Epoch 3/10\n",
            "870/870 [==============================] - 139s 160ms/step - loss: 0.0547 - accuracy: 0.9828\n",
            "Epoch 4/10\n",
            "870/870 [==============================] - 139s 160ms/step - loss: 0.0377 - accuracy: 0.9883\n",
            "Epoch 5/10\n",
            "870/870 [==============================] - 139s 160ms/step - loss: 0.0304 - accuracy: 0.9906\n",
            "Epoch 6/10\n",
            "870/870 [==============================] - 138s 159ms/step - loss: 0.0277 - accuracy: 0.9915\n",
            "Epoch 7/10\n",
            "870/870 [==============================] - 138s 159ms/step - loss: 0.0178 - accuracy: 0.9947\n",
            "Epoch 8/10\n",
            "870/870 [==============================] - 137s 158ms/step - loss: 0.0189 - accuracy: 0.9945\n",
            "Epoch 9/10\n",
            "870/870 [==============================] - 139s 159ms/step - loss: 0.0181 - accuracy: 0.9941\n",
            "Epoch 10/10\n",
            "870/870 [==============================] - 139s 159ms/step - loss: 0.0143 - accuracy: 0.9959\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7aacd03e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ws9n0IS_bfEQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "d91dfd38-37b5-4483-b428-c02fa61e9484"
      },
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "\n",
        "for pic in test_names:\n",
        "\n",
        "  # predicting images\n",
        "  path = test_dir + '/' + pic\n",
        "  img = image.load_img(path, target_size=(200, 200, 3))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0) # I'm not sure why you need this part\n",
        "\n",
        "  y_prob = model.predict(x)\n",
        "  y_classes = y_prob.argmax(axis=-1)\n",
        "  #print(y_classes)\n",
        "  print('Actual: ' + str(pic) + '   Predicted: ' + sorted(train_names)[int(y_classes)])\n",
        "\n",
        "  #classes = np.argmax(model.predict(images), axis=-1)\n",
        "  #print(classes[0])\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual: J_test.jpg   Predicted: J\n",
            "Actual: N_test.jpg   Predicted: N\n",
            "Actual: P_test.jpg   Predicted: P\n",
            "Actual: X_test.jpg   Predicted: L\n",
            "Actual: A_test.jpg   Predicted: A\n",
            "Actual: T_test.jpg   Predicted: T\n",
            "Actual: Z_test.jpg   Predicted: Z\n",
            "Actual: G_test.jpg   Predicted: G\n",
            "Actual: H_test.jpg   Predicted: H\n",
            "Actual: C_test.jpg   Predicted: C\n",
            "Actual: Y_test.jpg   Predicted: Y\n",
            "Actual: W_test.jpg   Predicted: W\n",
            "Actual: E_test.jpg   Predicted: E\n",
            "Actual: space_test.jpg   Predicted: space\n",
            "Actual: U_test.jpg   Predicted: U\n",
            "Actual: K_test.jpg   Predicted: K\n",
            "Actual: D_test.jpg   Predicted: D\n",
            "Actual: M_test.jpg   Predicted: M\n",
            "Actual: S_test.jpg   Predicted: L\n",
            "Actual: B_test.jpg   Predicted: B\n",
            "Actual: R_test.jpg   Predicted: R\n",
            "Actual: L_test.jpg   Predicted: L\n",
            "Actual: V_test.jpg   Predicted: V\n",
            "Actual: I_test.jpg   Predicted: I\n",
            "Actual: nothing_test.jpg   Predicted: nothing\n",
            "Actual: F_test.jpg   Predicted: F\n",
            "Actual: O_test.jpg   Predicted: O\n",
            "Actual: Q_test.jpg   Predicted: Q\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9Cblro2od7J",
        "colab_type": "text"
      },
      "source": [
        "***Debugging***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nMlNeo0ewkm",
        "colab_type": "code",
        "outputId": "8b116106-7c4a-4807-ee51-483b37613065",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "#!ls '/tmp/'\n",
        "#!ls '/tmp/asl-alphabet/'\n",
        "#!ls '/tmp/asl_alphabet_train/'\n",
        "#!ls '/tmp/asl_alphabet_train/asl_alphabet_train'\n",
        "!ls '/tmp/asl_alphabet_test/asl_alphabet_test'"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A_test.jpg  G_test.jpg\tM_test.jpg\t  R_test.jpg\t  W_test.jpg\n",
            "B_test.jpg  H_test.jpg\tnothing_test.jpg  space_test.jpg  X_test.jpg\n",
            "C_test.jpg  I_test.jpg\tN_test.jpg\t  S_test.jpg\t  Y_test.jpg\n",
            "D_test.jpg  J_test.jpg\tO_test.jpg\t  T_test.jpg\t  Z_test.jpg\n",
            "E_test.jpg  K_test.jpg\tP_test.jpg\t  U_test.jpg\n",
            "F_test.jpg  L_test.jpg\tQ_test.jpg\t  V_test.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}